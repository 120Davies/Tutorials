{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Selenium and Beautiful Soup\n",
    "Selenium is a browser automation tool that can not only be used for testing, but also for many other purposes. It's especially useful because using it we can also scrape data that are client side rendered.\n",
    "\n",
    "Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Selenuim a WebDriver for your favorite web browser must also be installed. The Firefox WebDriver(GeckoDriver) can be installed by going to [this page](https://github.com/mozilla/geckodriver/releases/) and downloading the appropriate file for your operating system. After the download has finished the file has to be extracted.\n",
    "\n",
    "Now the file can either be [added to path](https://www.architectryan.com/2018/03/17/add-to-the-path-on-windows-10/) or copied into the working directory. I chose to copy it to my working directory because Iâ€™m not using it that often.\n",
    "\n",
    "Importing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# website url\n",
    "base_url = \"https://programmingwithgilbert.firebaseapp.com/\"\n",
    "videos_url = \"https://programmingwithgilbert.firebaseapp.com/videos/keras-tutorials\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to load the data using urllib. This won't get any data because it can't load data which is loaded after the document.onload function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "page = urllib.request.urlopen(videos_url)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a firefox session and navigate to the base url of the video section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firefox session\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(videos_url)\n",
    "driver.implicitly_wait(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To navigate to the specifc pages we nned to get the buttons which a a text of \"Watch\" and then navigate to each side, scrape the data, save it and go back to the main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical ',\n",
       "  'def getData(): Copy',\n",
       "  'def getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28     ',\n",
       "  '    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10) Copy',\n",
       "  '    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1) ',\n",
       "  '    plt.imshow(X_train[0][:,:,0])\\n    plt.show() Copy',\n",
       "  '    return X_train, y_train, X_test, y_test\\ngetData() ',\n",
       "  'import numpy as np import pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\ndef getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28\\n    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10)\\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\\n    plt.imshow(X_train[0][:,:,0])\\n    plt.show()\\n    return X_train, y_train, X_test, y_test\\n\\ngetData() Copy'],\n",
       " ['import numpy as np import pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\ndef getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28\\n    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10)\\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\\n    plt.imshow(X_train[0][:,:,0])\\n    plt.show()\\n    return X_train, y_train, X_test, y_test\\n\\ngetData() ',\n",
       "  'X_train, y_train, X_test, y_test = getData()',\n",
       "  'from keras.models import Sequential, model_from_json\\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom keras.optimizers import RMSprop\\nfrom keras.callbacks import ReduceLROnPlateau\\nimport os ',\n",
       "  'def trainModel(X_train, y_train, X_test, y_test):Copy',\n",
       "  '    batch_size = 64\\n    epochs = 15 ',\n",
       "  '    model = Sequential()\\n',\n",
       "  \"    model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25)) \",\n",
       "  \"    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25)) Copy\",\n",
       "  '    model.add(Flatten())',\n",
       "  \"    model.add(Dense(256, activation='relu'))\\n    model.add(Dropout(rate=0.5))\\n    model.add(Dense(10, activation='softmax')) \",\n",
       "  \"    optimizer = RMSprop(lr=0.001)\\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001) \",\n",
       "  '    datagen = ImageDataGenerator(\\n           rotation_range=10,\\n           zoom_range=0.1,\\n           width_shift_range=0.1,\\n           height_shift_range=0.1) Copy',\n",
       "  \"    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\\n    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs,\\n                                                       validation_data=(X_test, y_test), verbose=2,\\n                                                       steps_per_epoch=X_train.shape[0]//batch_size, \\n                                                       callbacks=[learning_rate_reduction]) \",\n",
       "  \"    model_json = model.to_json()\\n    with open('model.json', 'w') as json_file:\\n        json_file.write(model_json)\\n    model.save_weights('mnist_model.h5')\\n    return model \",\n",
       "  'def loadModel():\\n    json_file = open(\\'model.json\\', \\'r\\')\\n    model_json = json_file.read()\\n    json_file.close()\\n    model = model_from_json(model_json)\\n    model.load_weights(\"mnist_model.h5\")\\n    return model ',\n",
       "  \"if(not os.path.exists('mnist_model.h5')):\\n    model = trainModel(X_train, y_train, X_test, y_test)\\n    print('trained model')\\n    print(model.summary())\\nelse:\\n    model = loadModel()\\n    print('loaded model')\\n    print(model.summary()) Copy\",\n",
       "  'import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\nfrom keras.models import Sequential, model_from_json\\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom keras.optimizers import RMSprop\\nfrom keras.callbacks import ReduceLROnPlateau\\nimport os\\ndef getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28\\n    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10)\\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\\n    plt.imshow(X_train[0][:,:,0])\\n    plt.show()\\n    return X_train, y_train, X_test, y_test\\n\\ndef trainModel(X_train, y_train, X_test, y_test):\\n    # training parameters\\n    batch_size = 64\\n    epochs = 15\\n    # create model and add layers\\n    model = Sequential()\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\\'relu\\', input_shape=(28,28,1)))\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\\'relu\\'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\\'relu\\'))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\\'relu\\'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25))\\n    model.add(Flatten())\\n    model.add(Dense(256, activation=\\'relu\\'))\\n    model.add(Dropout(rate=0.5))\\n    model.add(Dense(10, activation=\\'softmax\\'))\\n    # define model optimizer and callback function\\n    optimizer = RMSprop(lr=0.001)\\n    learning_rate_reduction = ReduceLROnPlateau(monitor=\\'val_acc\\', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\\n    # Image Augmentation\\n    datagen = ImageDataGenerator(\\n             rotation_range=10,\\n             zoom_range=0.1,\\n             width_shift_range=0.1,\\n             height_shift_range=0.1)\\n     # compile model define loss, optimizer and metrics\\n    model.compile(loss=\\'categorical_crossentropy\\', optimizer=optimizer, metrics=[\\'accuracy\\'])\\n     # Train model\\n    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs,\\n                                   validation_data=(X_test, y_test), verbose=2, \\n \\t\\t\\t\\t  steps_per_epoch=X_train.shape[0]//batch_size, \\n \\t\\t\\t\\t  callbacks=[learning_rate_reduction])\\n     # Save model structure and weights\\n    model_json = model.to_json()\\n    with open(\\'model.json\\', \\'w\\') as json_file:\\n        json_file.write(model_json)\\n    model.save_weights(\\'mnist_model.h5\\')\\n    return model\\n\\ndef loadModel():\\n    json_file = open(\\'model.json\\', \\'r\\')\\n    model_json = json_file.read()\\n    json_file.close()\\n    model = model_from_json(model_json)\\n    model.load_weights(\"mnist_model.h5\")\\n    return model\\n\\nX_train, y_train, X_test, y_test = getData()\\n\\nif(not os.path.exists(\\'mnist_model.h5\\')):\\n    model = trainModel(X_train, y_train, X_test, y_test)\\n    print(\\'trained model\\')\\n    print(model.summary())\\nelse:\\n    model = loadModel()\\n    print(\\'loaded model\\')\\n    print(model.summary())\\n '],\n",
       " ['from tkinter import *\\nfrom PIL import ImageGrab\\nimport numpy as np\\nimport scipy.misc\\nfrom keras.models import Sequential, model_from_json\\nimport os\\nfrom keras_cnn import trainModel, getData\\n ',\n",
       "  'class Paint(object):Copy',\n",
       "  '    def __init__(self):\\n        self.root = Tk() ',\n",
       "  \"        self.c = Canvas(self.root, bg='white', width=280, height=280)\\n        self.c.grid(row=1, columnspan=5)\\n\\n        self.classify_button = Button(self.root, text='classify', command=lambda:self.classify(self.c))\\n        self.classify_button.grid(row=0, column=1)\\n\\n        self.clear = Button(self.root, text='clear', command=self.clear)\\n        self.clear.grid(row=0, column=3)\\n\\n        self.prediction_text = Text(self.root, height=2, width=10)\\n        self.prediction_text.grid(row=2, column=3)\\n Copy\",\n",
       "  \"    def setup(self):\\n        self.old_x = None\\n        self.old_y = None\\n        self.line_width = 15\\n        self.color = 'black'\\n        self.c.bind('', self.paint)\\n        self.c.bind('', self.reset)\\n \",\n",
       "  '        self.setup()\\n        self.root.mainloop() Copy',\n",
       "  'def paint(self, event):\\n        paint_color = self.color\\n        if self.old_x and self.old_y:\\n            self.c.create_line(self.old_x, self.old_y, event.x, event.y,\\n                               width=self.line_width, fill=paint_color,\\n                               capstyle=ROUND, smooth=TRUE, splinesteps=36)\\n        self.old_x = event.x\\n        self.old_y = event.y\\n ',\n",
       "  '    def clear(self):\\n        self.c.delete(\"all\") Copy',\n",
       "  '    def reset(self, event):\\n        self.old_x, self.old_y = None, None ',\n",
       "  '    def classify(self, widget):Copy',\n",
       "  '        x=self.root.winfo_rootx()+widget.winfo_x()\\n        y=self.root.winfo_rooty()+widget.winfo_y()\\n        x1=x+widget.winfo_width()\\n        y1=y+widget.winfo_height() ',\n",
       "  \"        ImageGrab.grab().crop((x,y,x1,y1)).resize((28, 28)).save('classify.png')\\n        img = scipy.misc.imread('classify.png', flatten=False, mode='P')\\n        img = np.array(img)\\n        img = np.reshape(img, (1, 28, 28, 1))\\n Copy\",\n",
       "  '        img[img==0] = 255\\n        img[img==225] = 0 ',\n",
       "  '        pred = self.model.predict([img])\\n        pred = np.argmax(pred)\\n        print(pred)\\n        self.prediction_text.delete(\"1.0\", END)\\n        self.prediction_text.insert(END, pred)\\n Copy',\n",
       "  '    def loadModel(self):\\n        # if model exists load it else create and train it\\n        if(os.path.exists(\\'mnist_model.h5\\')):\\n            print(\\'loading model\\')\\n            json_file = open(\\'model.json\\', \\'r\\')\\n            model_json = json_file.read()\\n            json_file.close()\\n            model = model_from_json(model_json)\\n            model.load_weights(\"mnist_model.h5\")\\n            return model\\n        else:\\n            print(\\'train model\\')\\n            X_train, y_train, X_test, y_test = getData()\\n            model = trainModel(X_train, y_train, X_test, y_test)\\n \\t   return model\\n ',\n",
       "  '        self.model = self.loadModel()Copy',\n",
       "  \"if __name__ == '__main__':\\n    Paint() \",\n",
       "  'from tkinter import *\\nfrom PIL import ImageGrab\\nimport numpy as np\\nimport scipy.misc\\nfrom keras.models import Sequential, model_from_json\\nimport os\\nfrom keras_cnn import trainModel, getData\\nclass Paint(object):\\n\\n    def __init__(self):\\n        self.root = Tk()\\n\\n        #defining Canvas\\n        self.c = Canvas(self.root, bg=\\'white\\', width=280, height=280)\\n        self.c.grid(row=1, columnspan=5)\\n\\n        self.classify_button = Button(self.root, text=\\'classify\\', command=lambda:self.classify(self.c))\\n        self.classify_button.grid(row=0, column=1)\\n\\n        self.clear = Button(self.root, text=\\'clear\\', command=self.clear)\\n        self.clear.grid(row=0, column=3)\\n\\n        self.prediction_text = Text(self.root, height=2, width=10)\\n        self.prediction_text.grid(row=2, column=3)\\n\\n        self.model = self.loadModel()\\n        self.setup()\\n        self.root.mainloop()\\n\\n    def setup(self):\\n        self.old_x = None\\n        self.old_y = None\\n        self.line_width = 15\\n        self.color = \\'black\\'\\n        self.c.bind(\\'\\', self.paint)\\n        self.c.bind(\\'\\', self.reset)\\n\\n    def clear(self):\\n        self.c.delete(\"all\")\\n\\n    def paint(self, event):\\n        paint_color = self.color\\n        if self.old_x and self.old_y:\\n            self.c.create_line(self.old_x, self.old_y, event.x, event.y,\\n                               width=self.line_width, fill=paint_color,\\n                               capstyle=ROUND, smooth=TRUE, splinesteps=36)\\n        self.old_x = event.x\\n        self.old_y = event.y\\n\\n    def reset(self, event):\\n        self.old_x, self.old_y = None, None\\n\\n    def classify(self, widget):\\n        #getting pixel information\\n        x=self.root.winfo_rootx()+widget.winfo_x()\\n        y=self.root.winfo_rooty()+widget.winfo_y()\\n        x1=x+widget.winfo_width()\\n        y1=y+widget.winfo_height()\\n        #save drawing\\n        ImageGrab.grab().crop((x,y,x1,y1)).resize((28, 28)).save(\\'classify.png\\')\\n        img = scipy.misc.imread(\\'classify.png\\', flatten=False, mode=\\'P\\')\\n        img = np.array(img)\\n        img = np.reshape(img, (1, 28, 28, 1))\\n        # Change pixels to work with our classifier\\n        img[img==0] = 255\\n        img[img==225] = 0\\n        # Predict digit\\n        pred = self.model.predict([img])\\n        # Get index with highest probability\\n        pred = np.argmax(pred)\\n        print(pred)\\n        self.prediction_text.delete(\"1.0\", END)\\n        self.prediction_text.insert(END, pred)\\n\\n    def loadModel(self):\\n        # if model exists load it else create and train it\\n        if(os.path.exists(\\'mnist_model.h5\\')):\\n            print(\\'loading model\\')\\n            json_file = open(\\'model.json\\', \\'r\\')\\n            model_json = json_file.read()\\n            json_file.close()\\n            model = model_from_json(model_json)\\n            model.load_weights(\"mnist_model.h5\")\\n            return model\\n        else:\\n            print(\\'train model\\')\\n            X_train, y_train, X_test, y_test = getData()\\n            model = trainModel(X_train, y_train, X_test, y_test)\\n \\t   return model\\n\\nif __name__ == \\'__main__\\':\\n    Paint() Copy'],\n",
       " ['from __future__ import print_function\\nfrom keras.callbacks import LambdaCallback\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation\\nfrom keras.layers import LSTM\\nfrom keras.optimizers import RMSprop\\nimport numpy as np\\nimport random\\nimport sys\\nimport io ',\n",
       "  \"text = open('sherlock_homes.txt', 'r').read().lower()\\nprint('text length', len(text))Copy\",\n",
       "  \"chars = sorted(list(set(text)))\\nprint('total chars: ', len(chars))\",\n",
       "  'char_indices = dict((c, i) for i, c in enumerate(chars))\\nindices_char = dict((i, c) for i, c in enumerate(chars))Copy',\n",
       "  \"maxlen = 40\\nstep = 3\\nsentences = []\\nnext_chars = []\\nfor i in range(0, len(text) - maxlen, step):\\n    sentences.append(text[i: i + maxlen])\\n    next_chars.append(text[i + maxlen])\\nprint('nb sequences:', len(sentences))\",\n",
       "  'x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\\nfor i, sentence in enumerate(sentences):\\n    for t, char in enumerate(sentence):\\n        x[i, t, char_indices[char]] = 1\\n    y[i, char_indices[next_chars[i]]] = 1 Copy',\n",
       "  \"model = Sequential()\\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\\nmodel.add(Dense(len(chars)))\\nmodel.add(Activation('softmax'))\\n\",\n",
       "  \"from keras.layers import Dropout\\nmodel = Sequential()\\nmodel.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\\nmodel.add(Dropout(0.25))\\nmodel.add(LSTM(256))\\nmodel.add(Dropout(0.25))\\nmodel.add(Dense(len(chars)))\\nmodel.add(Activation('softmax')) Copy\",\n",
       "  \"optimizer = RMSprop(lr=0.01)\\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\",\n",
       "  \"def sample(preds, temperature=1.0):\\n    # helper function to sample an index from a probability array\\n    preds = np.asarray(preds).astype('float64')\\n    preds = np.log(preds) / temperature\\n    exp_preds = np.exp(preds)\\n    preds = exp_preds / np.sum(exp_preds)\\n    probas = np.random.multinomial(1, preds, 1)\\n    return np.argmax(probas)Copy\",\n",
       "  'def on_epoch_end(epoch, logs):\\n    # Function invoked at end of each epoch. Prints generated text.\\n    print()\\n    print(\\'----- Generating text after Epoch: %d\\' % epoch)\\n\\n    start_index = random.randint(0, len(text) - maxlen - 1)\\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\\n        print(\\'----- diversity:\\', diversity)\\n\\n        generated = \\'\\'\\n        sentence = text[start_index: start_index + maxlen]\\n        generated += sentence\\n        print(\\'----- Generating with seed: \"\\' + sentence + \\'\"\\')\\n        sys.stdout.write(generated)\\n\\n        for i in range(400):\\n            x_pred = np.zeros((1, maxlen, len(chars)))\\n            for t, char in enumerate(sentence):\\n                x_pred[0, t, char_indices[char]] = 1.\\n\\n            preds = model.predict(x_pred, verbose=0)[0]\\n            next_index = sample(preds, diversity)\\n            next_char = indices_char[next_index]\\n\\n            generated += next_char\\n            sentence = sentence[1:] + next_char\\n\\n            sys.stdout.write(next_char)\\n            sys.stdout.flush()\\n        print()',\n",
       "  'print_callback = LambdaCallback(on_epoch_end=on_epoch_end)Copy',\n",
       "  'from keras.callbacks import ModelCheckpoint\\n\\nfilepath = \"weights.hdf5\"\\ncheckpoint = ModelCheckpoint(filepath, monitor=\\'loss\\',\\nverbose=1, save_best_only=True,\\nmode=\\'min\\')',\n",
       "  \"from keras.callbacks import ReduceLROnPlateau\\nreduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\\n                               patience=1, min_lr=0.001)Copy\",\n",
       "  'callbacks = [print_callback, checkpoint, reduce_lr]',\n",
       "  'model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)Copy',\n",
       "  \"def generate_text(length, diversity):\\n    # Get random starting text\\n    start_index = random.randint(0, len(text) - maxlen - 1)\\n    generated = ''\\n    sentence = text[start_index: start_index + maxlen]\\n    generated += sentence\\n    for i in range(length):\\n            x_pred = np.zeros((1, maxlen, len(chars)))\\n            for t, char in enumerate(sentence):\\n                x_pred[0, t, char_indices[char]] = 1.\\n\\n            preds = model.predict(x_pred, verbose=0)[0]\\n            next_index = sample(preds, diversity)\\n            next_char = indices_char[next_index]\\n\\n            generated += next_char\\n            sentence = sentence[1:] + next_char\\n    return generated\",\n",
       "  'print(generate_text(500, 0.2))Copy'],\n",
       " ['import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom keras.models import Sequential, load_model\\nfrom keras.layers import LSTM, Dense, Dropout\\nimport os ',\n",
       "  \"df = pd.read_csv('AAPL_data.csv')\\ndf.head() \",\n",
       "  \"df = df['open'].values\\ndf = df.reshape(-1, 1)\\nprint(df.shape)\\ndf[:5] \",\n",
       "  'dataset_train = np.array(df[:int(df.shape[0]*0.8)])\\ndataset_test = np.array(df[int(df.shape[0]*0.8)-50:])\\nprint(dataset_train.shape)\\nprint(dataset_test.shape) Copy',\n",
       "  'scaler = MinMaxScaler(feature_range=(0,1))\\ndataset_train = scaler.fit_transform(dataset_train)\\ndataset_train[:5] ',\n",
       "  'dataset_test = scaler.transform(dataset_test) dataset_test[:5]',\n",
       "  'def create_dataset(df):\\n    x = []\\n    y = []\\n    for i in range(50, df.shape[0]):\\n        x.append(df[i-50:i, 0])\\n        y.append(df[i, 0])\\n    x = np.array(x)\\n    y = np.array(y)\\n    return x,y ',\n",
       "  ' x_train, y_train = create_dataset(dataset_train)\\nx_train[:1] Copy',\n",
       "  'x_test, y_test = create_dataset(dataset_test)\\nx_test[:1] ',\n",
       "  '# Reshape features for LSTM Layer\\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1)) ',\n",
       "  'model = Sequential()\\nmodel.add(LSTM(units=96, return_sequences=True, input_shape=(x_train.shape[1], 1)))\\nmodel.add(Dropout(0.2))\\nmodel.add(LSTM(units=96, return_sequences=True))\\nmodel.add(Dropout(0.2))\\nmodel.add(LSTM(units=96, return_sequences=True))\\nmodel.add(Dropout(0.2))\\nmodel.add(LSTM(units=96))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(units=1)) ',\n",
       "  \"model.compile(loss='mean_squared_error', optimizer='adam')Copy\",\n",
       "  \"if(not os.path.exists('stock_prediction.h5')):\\n    model.fit(x_train, y_train, epochs=50, batch_size=32)\\n    model.save('stock_prediction.h5')\\n \",\n",
       "  \"model = load_model('stock_prediction.h5') \",\n",
       "  'predictions = model.predict(x_test)\\npredictions = scaler.inverse_transform(predictions)\\n\\nfig, ax = plt.subplots(figsize=(8,4))\\nplt.plot(df, color=\\'red\\',  label=\"True Price\")\\nax.plot(range(len(y_train)+50,len(y_train)+50+len(predictions)),\\npredictions, color=\\'blue\\', label=\\'Predicted Testing Price\\')\\nplt.legend() ',\n",
       "  \"y_test_scaled = scaler.inverse_transform(y_test.reshape(-1, 1))\\n\\nfig, ax = plt.subplots(figsize=(8,4))\\nax.plot(y_test_scaled, color='red', label='True Testing Price')\\nplt.plot(predictions, color='blue', label='Predicted Testing Price')\\nplt.legend() Copy\"],\n",
       " ['import re\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nimport matplotlib.pyplot as plt\\n\\nfrom keras.models import Sequential, load_model\\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences ',\n",
       "  \"data = pd.read_csv('Tweets.csv')\\ndata = data.sample(frac=1).reset_index(drop=True)\\nprint(data.shape)\\ndata.head() \",\n",
       "  \"data = data[['airline_sentiment', 'text']]\\ndata.head() \",\n",
       "  \"data['airline_sentiment'].value_counts().sort_index().plot.bar() Copy\",\n",
       "  \"data['text'].str.len().plot.hist() \",\n",
       "  \"data['text'].apply(lambda x: x.lower())\\n#transform text to lowercase\\ndata['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-z0-9\\\\s]', '', x))\\ndata['text'].head() \",\n",
       "  'tokenizer = Tokenizer(num_words=5000, split=\" \")\\ntokenizer.fit_on_texts(data[\\'text\\'].values)\\n\\nX = tokenizer.texts_to_sequences(data[\\'text\\'].values)\\nX = pad_sequences(X) # padding our text vector so they all have the same length\\nX[:5] ',\n",
       "  \"model = Sequential()\\nmodel.add(Embedding(5000, 256, input_length=X.shape[1]))\\nmodel.add(Dropout(0.3))\\nmodel.add(LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))\\nmodel.add(LSTM(256, dropout=0.3, recurrent_dropout=0.2))\\nmodel.add(Dense(3, activation='softmax'))\\n Copy\",\n",
       "  \"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.summary() \",\n",
       "  \"y = pd.get_dummies(data['airline_sentiment']).values\\n[print(data['airline_sentiment'][i], y[i]) for i in range(0,5)] \",\n",
       "  'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) ',\n",
       "  'batch_size = 32\\nepochs = 8\\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2) Copy',\n",
       "  \"model.save('sentiment_analysis.h5')\",\n",
       "  \"predictions = model.predict(X_test)\\n[print(data['text'][i], predictions[i], y_test[i]) for i in range(0, 5)] \",\n",
       "  \"pos_count, neu_count, neg_count = 0, 0, 0\\nreal_pos, real_neu, real_neg = 0, 0, 0\\nfor i, prediction in enumerate(predictions):\\n    if np.argmax(prediction)==2:\\n        pos_count += 1\\n    elif np.argmax(prediction)==1:\\n        neu_count += 1\\n    else:\\n        neg_count += 1\\n\\n        if np.argmax(y_test[i])==2:\\n        real_pos += 1\\n    elif np.argmax(y_test[i])==1:\\n        real_neu += 1\\n    else:\\n        real_neg +=1\\n\\nprint('Positive predictions:', pos_count)\\nprint('Neutral predictions:', neu_count)\\nprint('Negative predictions:', neg_count)\\nprint('Real positive:', real_pos)\\nprint('Real neutral:', real_neu)\\nprint('Real negative:', real_neg) \"],\n",
       " ['import numpy as np\\nimport matplotlib.pyplot as plt\\n\\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, LSTM\\nfrom keras.models import Model\\nfrom keras.datasets import mnist ',\n",
       "  \"(x_train, y_train), (x_test, y_test) = mnist.load_data()\\ndel y_train\\ndel y_test\\n\\n# scale data between 0 and 1\\nx_train = x_train.astype('float32') / 256\\nx_test = x_test.astype('float32') / 256\\n\\n# Reshape data\\nx_train = x_train.reshape(len(x_train), 28*28)\\nx_test = x_test.reshape(len(x_test), 28*28)\\n\\nprint('Training data shape:', x_train.shape)\\nprint('Testing data shape:', x_test.shape) Copy\",\n",
       "  \"input_layer = Input(shape=(784,))\\n# encoded layer. compresses data by the factor 12.25 (784/64=12.25)\\nencoded = Dense(64, activation='relu')(input_layer)\\n# decoded layer. reconstructs the input (units are equal to the input dimension of the image)\\ndecoded = Dense(784, activation='sigmoid')(encoded) \",\n",
       "  '# this model maps an input to its reconstruction\\nautoencoder = Model(input_layer, decoded) Copy',\n",
       "  '# this model maps an input to its encoded representation\\nencoder = Model(input_layer, encoded) ',\n",
       "  '# for the decoder we need to create an input layer which has the dimensionality of the encoded layer\\ninput_layer_dec = Input(shape=(64,))\\n# retrieve the last layer of the autoencoder model and place it after the input layer\\ndecoder_layer = autoencoder.layers[-1](input_layer_dec)\\n\\ndecoder = Model(input_layer_dec, decoder_layer) Copy',\n",
       "  \"autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\\n\\nautoencoder.fit(x_train, x_train, epochs=50, batch_size=256,\\n                shuffle=True,validation_data=(x_test, x_test)) \",\n",
       "  'encoded_images = encoder.predict(x_test)\\ndecoded_images = decoder.predict(encoded_images) # takes the output from the encoder as input Copy',\n",
       "  'def visualize_data(data, size):\\n    n = 10 # number of digits that will be displayed\\n    plt.figure(figsize=(20, 4))\\n    plt.gray()\\n    for i in range(n):\\n        ax = plt.subplot(2, n, i+1)\\n        plt.imshow(data[i].reshape(size, size)) # reshape the image back to its normal shape\\n        # disable axis\\n        ax.get_xaxis().set_visible(False)\\n        ax.get_yaxis().set_visible(False)\\n    plt.show()     ',\n",
       "  'visualize_data(x_test, 28)\\nvisualize_data(decoded_images, 28) Copy',\n",
       "  'visualize_data(encoded_images, 8) ',\n",
       "  \"input_layer = Input(shape=(784,))\\n encoded = Dense(128, activation='relu')(input_layer)\\nencoded = Dense(64, activation='relu')(encoded)\\n\\ndecoded = Dense(128, activation='relu')(encoded)\\ndecoded = Dense(784, activation='sigmoid')(decoded) Copy\",\n",
       "  \"autoencoder = Model(input_layer, decoded)\\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\\n\\nautoencoder.fit(x_train, x_train, epochs=100, batch_size=256,\\n                shuffle=True,validation_data=(x_test, x_test)) \",\n",
       "  'autoencoded_images = autoencoder.predict(x_test) Copy',\n",
       "  'visualize_data(x_test, 28)\\nvisualize_data(autoencoded_images, 28) ',\n",
       "  \"# Reshape data for Convolutional Layer\\nx_train = x_train.reshape(len(x_train), 28, 28, 1)\\nx_test = x_test.reshape(len(x_test), 28, 28, 1)\\nprint('Training data shape:', x_train.shape)\\nprint('Testing data shape:', x_test.shape) Copy\",\n",
       "  \"input_layer = Input(shape=(28, 28, 1))\\n\\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)\\nx = MaxPooling2D((2, 2), padding='same')(x)\\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\\nx = MaxPooling2D((2, 2), padding='same')(x)\\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\\nencoded = MaxPooling2D((2,2), padding='same')(x)\\n\\n# encoded representation is (4, 4, 8) i.e. 128-dimensional\\n\\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\\nx = UpSampling2D((2, 2))(x)\\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\\nx = UpSampling2D((2, 2))(x)\\nx = Conv2D(16, (3, 3), activation='relu')(x)\\nx = UpSampling2D((2, 2))(x)\\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x) \",\n",
       "  \"autoencoder = Model(input_layer, decoded)\\nautoencoder.compile(loss='binary_crossentropy', optimizer='adadelta') Copy\",\n",
       "  'autoencoder.fit(x_train, x_train, epochs=50,\\n                batch_size=128, shuffle=True,validation_data=(x_test, x_test)) ',\n",
       "  'autoencoded_images = autoencoder.predict(x_test)Copy',\n",
       "  'visualize_data(x_test, 28)\\nvisualize_data(autoencoded_images, 28) ',\n",
       "  'noise_factor = 0.5\\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\\n\\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\\nx_test_noisy = np.clip(x_test_noisy, 0., 1.) Copy',\n",
       "  'visualize_data(x_test_noisy, 28)',\n",
       "  \"input_layer = Input(shape=(28, 28, 1))\\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\\nx = MaxPooling2D((2, 2), padding='same')(x)\\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\\nencoded = MaxPooling2D((2, 2), padding='same')(x)\\n\\n# at this point the representation is (7, 7, 32)\\n\\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\\nx = UpSampling2D((2, 2))(x)\\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\\nx = UpSampling2D((2, 2))(x)\\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x) Copy\",\n",
       "  \"autoencoder = Model(input_layer, decoded)\\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy') \",\n",
       "  'autoencoder.fit(x_train_noisy, x_train, epochs=100,batch_size=128,\\n                shuffle=True,validation_data=(x_test_noisy, x_test))Copy',\n",
       "  'denoised_data = autoencoder.predict(x_train_noisy)\\nvisualize_data(x_train_noisy, 28)\\nvisualize_data(denoised_data, 28) '],\n",
       " [\"import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom keras.utils import to_categorical\\n\\nnp.random.seed(0)\\n\\niris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\\n                   names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label'])\\nle = LabelEncoder()\\niris['label'] = le.fit_transform(iris['label'])\\nX = np.array(iris.drop(['label'], axis=1))\\ny = np.array(iris['label'])\\ny = to_categorical(y, num_classes=3)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\niris.head() \",\n",
       "  \"from keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\n\\ndef c_model():\\n    model = Sequential()\\n    model.add(Dense(32, activation='relu'))\\n    model.add(Dense(16, activation='relu'))\\n    model.add(Dense(3, activation='softmax'))\\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n    return model\\n\\nmodel = KerasClassifier(build_fn=c_model, epochs=50, batch_size=32) \",\n",
       "  'model.fit(X_train, y_train)',\n",
       "  \"from sklearn.model_selection import GridSearchCV\\n\\nmodel = KerasClassifier(build_fn=c_model)\\n\\nbatch_sizes = [10, 20, 50, 100]\\nepochs = [5, 10, 50]\\nparameters = {'batch_size': batch_sizes, 'epochs': epochs}\\nclf = GridSearchCV(model, parameters)\\nclf.fit(X_train, y_train) Copy\",\n",
       "  \"print(clf.best_score_, clf.best_params_)\\nmeans = clf.cv_results_['mean_test_score']\\nparameters = clf.cv_results_['params']\\nfor mean, parammeter in zip(means, parameters):\\n    print(mean, parammeter) \",\n",
       "  \"def c_model(optimizer):\\n    model = Sequential()\\n    model.add(Dense(32, activation='relu'))\\n    model.add(Dense(16, activation='relu'))\\n    model.add(Dense(3, activation='softmax'))\\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\\n    return model\\n\\nmodel = KerasClassifier(build_fn=c_model, epochs=50, batch_size=32)\\nparameters = {'optimizer':['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']}\\nclf = GridSearchCV(model, parameters)\\nclf.fit(X_train, y_train) \",\n",
       "  \"print(clf.best_score_, clf.best_params_)\\nmeans = clf.cv_results_['mean_test_score']\\nparameters = clf.cv_results_['params']\\nfor mean, parammeter in zip(means, parameters):\\n    print(mean, parammeter) \",\n",
       "  \"def c_model(activation):\\n    model = Sequential()\\n    model.add(Dense(32, activation=activation))\\n    model.add(Dense(16, activation=activation))\\n    model.add(Dense(3, activation='softmax'))\\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n    return model\\n\\nmodel = KerasClassifier(build_fn=c_model, epochs=50, batch_size=32)\\nparameters = {'activation':['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']}\\nclf = GridSearchCV(model, parameters)\\nclf.fit(X_train, y_train) Copy\",\n",
       "  \"print(clf.best_score_, clf.best_params_)\\nmeans = clf.cv_results_['mean_test_score']\\nparameters = clf.cv_results_['params']\\nfor mean, parammeter in zip(means, parameters):\\n    print(mean, parammeter) \"],\n",
       " [\"from keras.applications.resnet50 import ResNet50\\nfrom keras.preprocessing import image\\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\\nimport numpy as np\\n\\n# load model\\nmodel = ResNet50(weights='imagenet') \",\n",
       "  \"img_path = 'elephant.jpg'\\nimg = image.load_img(img_path, target_size=(224, 224))\\nx = image.img_to_array(img)\\nx = np.expand_dims(x, axis=0)\\nx = preprocess_input(x) \",\n",
       "  \"prediction = model.predict(x)\\nprint('Predicted:', decode_predictions(prediction, top=5)[0]) \",\n",
       "  \"from keras.models import Model\\nfrom keras.applications.vgg19 import VGG19\\n\\nbase_model = VGG19(weights='imagenet')\\nbase_model.summary() Copy\",\n",
       "  \"model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\\nmodel.summary() \",\n",
       "  'block5_pool_features = model.predict(x)\\nblock5_pool_features ',\n",
       "  'from keras.layers import Flatten, Dense, Dropout\\n\\nbase_model = VGG19(weights = \"imagenet\", include_top=False, input_shape = (256, 256, 3))\\n\\nfor layer in base_model.layers[:5]:\\n    layer.trainable = False\\n\\n# Adding custom layers\\nx = base_model.output\\nx = Flatten()(x)\\n x = Dense(2048, activation=\\'relu\\')(x)\\nx = Dropout(0.3)(x)\\nx = Dense(1024, activation=\\'relu\\')(x)\\noutput = Dense(10, activation=\\'softmax\\')(x)\\n\\nmodel = Model(inputs=base_model.input, outputs=output)\\nmodel.summary() ']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_links = len(driver.find_elements_by_link_text('Watch'))\n",
    "code_blocks = []\n",
    "for i in range(num_links):\n",
    "    # navigate to link\n",
    "    button = driver.find_elements_by_class_name(\"btn-primary\")[i]\n",
    "    button.click()\n",
    "    # get soup\n",
    "    element = WebDriverWait(driver, 10).until(lambda x: x.find_element_by_id('iframe_container'))\n",
    "    tutorial_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tutorial_code_soup = tutorial_soup.find_all('div', attrs={'class': 'code-toolbar'})\n",
    "    tutorial_code = [i.getText() for i in tutorial_code_soup]\n",
    "    code_blocks.append(tutorial_code)\n",
    "    # go back to initial page\n",
    "    driver.execute_script(\"window.history.go(-1)\")\n",
    "code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import numpy as np import pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\ndef getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28\\n    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10)\\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\\n    plt.imshow(X_train[0][:,:,0])\\n    plt.show()\\n    return X_train, y_train, X_test, y_test\\n\\ngetData() ',\n",
       " 'X_train, y_train, X_test, y_test = getData()',\n",
       " 'from keras.models import Sequential, model_from_json\\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom keras.optimizers import RMSprop\\nfrom keras.callbacks import ReduceLROnPlateau\\nimport os ',\n",
       " 'def trainModel(X_train, y_train, X_test, y_test):Copy',\n",
       " '    batch_size = 64\\n    epochs = 15 ',\n",
       " '    model = Sequential()\\n',\n",
       " \"    model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25)) \",\n",
       " \"    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25)) Copy\",\n",
       " '    model.add(Flatten())',\n",
       " \"    model.add(Dense(256, activation='relu'))\\n    model.add(Dropout(rate=0.5))\\n    model.add(Dense(10, activation='softmax')) \",\n",
       " \"    optimizer = RMSprop(lr=0.001)\\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001) \",\n",
       " '    datagen = ImageDataGenerator(\\n           rotation_range=10,\\n           zoom_range=0.1,\\n           width_shift_range=0.1,\\n           height_shift_range=0.1) Copy',\n",
       " \"    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\\n    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs,\\n                                                       validation_data=(X_test, y_test), verbose=2,\\n                                                       steps_per_epoch=X_train.shape[0]//batch_size, \\n                                                       callbacks=[learning_rate_reduction]) \",\n",
       " \"    model_json = model.to_json()\\n    with open('model.json', 'w') as json_file:\\n        json_file.write(model_json)\\n    model.save_weights('mnist_model.h5')\\n    return model \",\n",
       " 'def loadModel():\\n    json_file = open(\\'model.json\\', \\'r\\')\\n    model_json = json_file.read()\\n    json_file.close()\\n    model = model_from_json(model_json)\\n    model.load_weights(\"mnist_model.h5\")\\n    return model ',\n",
       " \"if(not os.path.exists('mnist_model.h5')):\\n    model = trainModel(X_train, y_train, X_test, y_test)\\n    print('trained model')\\n    print(model.summary())\\nelse:\\n    model = loadModel()\\n    print('loaded model')\\n    print(model.summary()) Copy\",\n",
       " 'import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\nfrom keras.models import Sequential, model_from_json\\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom keras.optimizers import RMSprop\\nfrom keras.callbacks import ReduceLROnPlateau\\nimport os\\ndef getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28\\n    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10)\\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\\n    plt.imshow(X_train[0][:,:,0])\\n    plt.show()\\n    return X_train, y_train, X_test, y_test\\n\\ndef trainModel(X_train, y_train, X_test, y_test):\\n    # training parameters\\n    batch_size = 64\\n    epochs = 15\\n    # create model and add layers\\n    model = Sequential()\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\\'relu\\', input_shape=(28,28,1)))\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\\'relu\\'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\\'relu\\'))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\\'relu\\'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25))\\n    model.add(Flatten())\\n    model.add(Dense(256, activation=\\'relu\\'))\\n    model.add(Dropout(rate=0.5))\\n    model.add(Dense(10, activation=\\'softmax\\'))\\n    # define model optimizer and callback function\\n    optimizer = RMSprop(lr=0.001)\\n    learning_rate_reduction = ReduceLROnPlateau(monitor=\\'val_acc\\', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\\n    # Image Augmentation\\n    datagen = ImageDataGenerator(\\n             rotation_range=10,\\n             zoom_range=0.1,\\n             width_shift_range=0.1,\\n             height_shift_range=0.1)\\n     # compile model define loss, optimizer and metrics\\n    model.compile(loss=\\'categorical_crossentropy\\', optimizer=optimizer, metrics=[\\'accuracy\\'])\\n     # Train model\\n    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs,\\n                                   validation_data=(X_test, y_test), verbose=2, \\n \\t\\t\\t\\t  steps_per_epoch=X_train.shape[0]//batch_size, \\n \\t\\t\\t\\t  callbacks=[learning_rate_reduction])\\n     # Save model structure and weights\\n    model_json = model.to_json()\\n    with open(\\'model.json\\', \\'w\\') as json_file:\\n        json_file.write(model_json)\\n    model.save_weights(\\'mnist_model.h5\\')\\n    return model\\n\\ndef loadModel():\\n    json_file = open(\\'model.json\\', \\'r\\')\\n    model_json = json_file.read()\\n    json_file.close()\\n    model = model_from_json(model_json)\\n    model.load_weights(\"mnist_model.h5\")\\n    return model\\n\\nX_train, y_train, X_test, y_test = getData()\\n\\nif(not os.path.exists(\\'mnist_model.h5\\')):\\n    model = trainModel(X_train, y_train, X_test, y_test)\\n    print(\\'trained model\\')\\n    print(model.summary())\\nelse:\\n    model = loadModel()\\n    print(\\'loaded model\\')\\n    print(model.summary())\\n ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_blocks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scraping all the needed data we can close the browser session and save the results into .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()\n",
    "for i, tutorial_code in enumerate(code_blocks):\n",
    "    with open('code_blocks{}.txt'.format(i), 'w') as f:\n",
    "        for code_block in tutorial_code:\n",
    "            f.write(code_block+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
